#!/usr/bin/env python3
"""
Basit test - Google ADK olmadan Ã§alÄ±ÅŸan sÃ¼rÃ¼m
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Any
import time
from loguru import logger
from urllib.parse import urljoin

# Site konfigÃ¼rasyonu
TRENDYOL_CONFIG = {
    "name": "trendyol",
    "base_url": "https://www.trendyol.com",
    "category_path": "/kozmetik-x-c40",
    "product_link_selector": "div.p-card-wrppr a",
    "rate_limit": 3.0
}

class SimpleCosmeticExtractor:
    """Basit kozmetik Ã¼rÃ¼n Ã§Ä±karÄ±cÄ±"""
    
    def __init__(self):
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def discover_urls(self, max_products: int = 5) -> List[str]:
        """ÃœrÃ¼n URL'lerini keÅŸfet"""
        logger.info(f"ğŸ” Trendyol'dan {max_products} Ã¼rÃ¼n URL'si arÄ±yorum...")
        
        try:
            url = TRENDYOL_CONFIG["base_url"] + TRENDYOL_CONFIG["category_path"]
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            async with self.session.get(url, headers=headers, timeout=30) as response:
                if response.status != 200:
                    logger.error(f"âŒ Sayfa yÃ¼klenemedi: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'lxml')
                
                # ÃœrÃ¼n linklerini bul
                links = []
                for link in soup.select(TRENDYOL_CONFIG["product_link_selector"])[:max_products]:
                    href = link.get("href")
                    if href:
                        full_url = urljoin(TRENDYOL_CONFIG["base_url"], href)
                        links.append(full_url)
                
                logger.info(f"âœ… {len(links)} Ã¼rÃ¼n URL'si bulundu")
                return links
                
        except Exception as e:
            logger.error(f"âŒ URL keÅŸfi hatasÄ±: {e}")
            return []
    
    async def extract_product_info(self, url: str) -> Dict[str, Any]:
        """Temel Ã¼rÃ¼n bilgilerini Ã§Ä±kar"""
        logger.info(f"ğŸ“¦ ÃœrÃ¼n bilgisi Ã§Ä±karÄ±lÄ±yor: {url}")
        
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            async with self.session.get(url, headers=headers, timeout=30) as response:
                if response.status != 200:
                    return {"error": f"HTTP {response.status}"}
                
                html = await response.text()
                soup = BeautifulSoup(html, 'lxml')
                
                # Temel bilgileri Ã§Ä±kar
                product = {
                    "url": url,
                    "name": self._extract_text(soup, "h1"),
                    "price": self._extract_text(soup, ".prc-dsc, .prc-slg"),
                    "description": self._extract_text(soup, ".product-desc, .detail-desc"),
                    "brand": self._extract_text(soup, ".brand, .product-brand")
                }
                
                # BoÅŸ deÄŸerleri temizle
                product = {k: v for k, v in product.items() if v}
                
                if product.get("name"):
                    logger.info(f"âœ… ÃœrÃ¼n Ã§Ä±karÄ±ldÄ±: {product['name'][:50]}...")
                    return product
                else:
                    logger.warning(f"âš ï¸ ÃœrÃ¼n adÄ± bulunamadÄ±: {url}")
                    return {"error": "ÃœrÃ¼n adÄ± bulunamadÄ±"}
                
        except Exception as e:
            logger.error(f"âŒ ÃœrÃ¼n Ã§Ä±karma hatasÄ± {url}: {e}")
            return {"error": str(e)}
    
    def _extract_text(self, soup: BeautifulSoup, selector: str) -> str:
        """CSS selector ile metin Ã§Ä±kar"""
        try:
            element = soup.select_one(selector)
            return element.get_text(strip=True) if element else ""
        except:
            return ""
    
    def generate_simple_seo(self, product: Dict[str, Any]) -> Dict[str, Any]:
        """Basit SEO verileri Ã¼ret"""
        name = product.get("name", "")
        description = product.get("description", "")
        
        # Basit keyword Ã§Ä±karma
        keywords = []
        text = (name + " " + description).lower()
        
        # Kozmetik terimleri
        cosmetic_terms = [
            "serum", "krem", "maske", "temizleyici", "tonik", "yaÄŸ",
            "cilt", "yÃ¼z", "gÃ¶z", "nemlendirici", "anti-aging", "vitamin"
        ]
        
        for term in cosmetic_terms:
            if term in text:
                keywords.append(term)
        
        # Marka ekle
        if product.get("brand"):
            keywords.insert(0, product["brand"].lower())
        
        # SEO baÅŸlÄ±k
        title = name[:60] if name else "Kozmetik ÃœrÃ¼n"
        
        # Meta aÃ§Ä±klama
        meta_desc = description[:160] if description else f"{name} - Kozmetik Ã¼rÃ¼nÃ¼"
        
        return {
            "keywords": keywords[:10],
            "primary_keyword": keywords[0] if keywords else "kozmetik",
            "title": title,
            "meta_description": meta_desc,
            "slug": self._create_slug(name)
        }
    
    def _create_slug(self, text: str) -> str:
        """URL slug oluÅŸtur"""
        if not text:
            return "urun"
        
        import re
        slug = text.lower()
        slug = re.sub(r'[^a-z0-9\s-]', '', slug)
        slug = re.sub(r'\s+', '-', slug)
        slug = slug[:50]
        return slug or "urun"


async def run_simple_test():
    """Basit test Ã§alÄ±ÅŸtÄ±r"""
    logger.info("ğŸš€ Basit Kozmetik SEO Ã‡Ä±karÄ±cÄ± Testi BaÅŸlÄ±yor")
    logger.info("Bu test ADK olmadan Ã§alÄ±ÅŸan basit bir sÃ¼rÃ¼mdÃ¼r\n")
    
    async with SimpleCosmeticExtractor() as extractor:
        # 1. URL'leri keÅŸfet
        urls = await extractor.discover_urls(3)
        
        if not urls:
            print("âŒ HiÃ§ Ã¼rÃ¼n URL'si bulunamadÄ±")
            return
        
        print(f"ğŸ” {len(urls)} Ã¼rÃ¼n URL'si bulundu")
        
        # 2. Her Ã¼rÃ¼nÃ¼ iÅŸle
        results = []
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ“¦ ÃœrÃ¼n {i}/{len(urls)} iÅŸleniyor...")
            
            # ÃœrÃ¼n bilgilerini Ã§Ä±kar
            product = await extractor.extract_product_info(url)
            
            if "error" in product:
                print(f"âŒ Hata: {product['error']}")
                continue
            
            # SEO verileri Ã¼ret
            seo_data = extractor.generate_simple_seo(product)
            
            result = {
                "product": product,
                "seo": seo_data
            }
            results.append(result)
            
            # Sonucu gÃ¶ster
            print(f"âœ… ÃœrÃ¼n: {product.get('name', 'Bilinmeyen')[:40]}...")
            print(f"   ğŸ·ï¸ Marka: {product.get('brand', 'Bilinmeyen')}")
            print(f"   ğŸ’° Fiyat: {product.get('price', 'Bilinmeyen')}")
            print(f"   ğŸ¯ Ana Keyword: {seo_data.get('primary_keyword', 'N/A')}")
            print(f"   ğŸ“ Keyword SayÄ±sÄ±: {len(seo_data.get('keywords', []))}")
            
            # Rate limiting
            await asyncio.sleep(3)
        
        # Ã–zet
        print(f"\n" + "="*60)
        print(f"ğŸ“Š TEST SONUÃ‡LARI")
        print(f"="*60)
        print(f"ğŸ” KeÅŸfedilen URL: {len(urls)}")
        print(f"âœ… Ä°ÅŸlenen ÃœrÃ¼n: {len(results)}")
        print(f"ğŸ“ˆ BaÅŸarÄ± OranÄ±: {len(results)/len(urls)*100:.1f}%")
        
        if results:
            print(f"\nğŸ“‹ Ä°ÅLENEN ÃœRÃœNLER:")
            for i, result in enumerate(results, 1):
                product = result["product"]
                seo = result["seo"]
                print(f"{i}. {product.get('name', 'Bilinmeyen')[:50]}")
                print(f"   Keywords: {', '.join(seo.get('keywords', [])[:5])}")
        
        print(f"\nâœ… Basit test tamamlandÄ±!")
        print(f"ğŸ“Œ Bu test ADK olmadan Ã§alÄ±ÅŸan temel bir sÃ¼rÃ¼mdÃ¼r.")
        print(f"ğŸš€ GeliÅŸmiÅŸ Ã¶zellikler iÃ§in ADK tabanlÄ± sistem kullanÄ±n.")
        
        return results


if __name__ == "__main__":
    # Logging ayarla
    logger.add("logs/simple_test.log", level="INFO")
    
    try:
        asyncio.run(run_simple_test())
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Test kullanÄ±cÄ± tarafÄ±ndan durduruldu")
    except Exception as e:
        logger.error(f"Test hatasÄ±: {e}")
        print(f"ğŸ’¥ Test hatasÄ±: {e}")